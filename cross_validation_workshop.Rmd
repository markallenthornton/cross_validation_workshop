---
title: "Cross-validation workshop"
author: "Mark A. Thornton, Ph. D."
date: "October 11, 2018"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction to cross-validation

Cross-validation represents a class of powerful procedures for measuring the performance of statistical models. In cross-validation,
a sub-sample of a data set is used to "train" a model - e.g., setting parameters - and this trained model is then used to made predictions
in another, independent subsample of the data. By training and testing in independent data, cross-validation provides an estimate of
the generalization performance of a model - how well it would work when applied to an entirely new data set of the same type. This 
helps to minimize the risk of trusting an "overfitted" model which captures a particular data set unrealistically well by capitalizing on chance. This workshop will describe how cross-validation work, with real and artificial examples examined in R.

## Why is cross-validation important?

When fitting a model to a particular data set, it can be hard to tell whether the observed explanatory power of the model (e.g., variance explained, or average error) would generalize to a new, similar data set. Why might a model not generalize from one data set to another? If the model is sufficiently complex, it can capitalize on chance to achieve good performance in the data set to which it is fit. This is known as "overfitting" - let's take a look at the classic example of polynomial regression:

```{r, echo=F, results="hide", include=F}
# load packages
if(!require(MASS)) install.packages("MASS"); require(MASS)

```

```{r fig.height=8, fig.width=8}
set.seed(1)
x <- runif(20)-.5
y <- x^2 + rnorm(20,0,.05)
fr <- seq(-.5,.5,by = .001)
layout(matrix(1:4,2,2,byrow = T))
plot(x,y,main="Raw data",pch=19,cex=1.25)
plot(x,y,main="Linear (y=bx) - underfit",pch=19,cex=1.25)
fit <- lm(y~x)
abline(fit,col="red",lwd=1.5)
sqrt(mean(fit$residuals^2))
plot(x,y,main="Quadratic (y=bx^2) - good fit",pch=19,cex=1.25)
fit <- lm(y~x+I(x^2))
sqrt(mean(fit$residuals^2))
fv <- predict(fit,data.frame(x=fr))
lines(fr,fv, col='red', type='l',lwd=1.5) 
plot(x,y,main="High order polynomial (y=bx^9) - overfit",pch=19,cex=1.25)
fit <- lm(y~poly(x,9))
fv <- predict(fit,data.frame(x=fr))
lines(fr,fv, col='red', type='l',lwd=1.5)
sqrt(mean(fit$residuals^2))
```

Here we can see simulated data where the underlying data-generating process is quadratic - the values of y are proportional to x-squared. A linear model completely fails to capture this relationship - what we could call "underfitting" - so we need polynomial regression (with coefficient 2) to achieve a good fit. However, what happens if we make the model too power/complex/highly parameterized? A 9th-degree polynomial appears to fit the data even better, passing very close to almost every point. We can quantify this performance in terms of RMSE - root mean square error. The RMSE of the linear model is .078, quadratic model is .044, and the 9th-order polynomial is .03 (smaller RMSE is better). However, this model is overfitted to the particular data in question, as we can see when we examine new data generated by the same process, as illustrated below.

```{r fig.height=4, fig.width=8}
set.seed(2)
x2 <- runif(20)-.5
y2 <- x2^2 + rnorm(20,0,.05)
layout(matrix(1:2,1,2,byrow = T))
plot(x,y,main="Quadratic (y=bx^2)",pch=19,cex=1.25)
fit <- lm(y~x+I(x^2))
fv <- predict(fit,data.frame(x=fr))
x2f <- predict(fit,data.frame(x=x2))
sqrt(mean((x2f-y2)^2))
lines(fr,fv, col='red', type='l',lwd=1.5)
points(x2,y2,col="blue",pch=19,cex=1.25)
segments(x2,y2,x2,x2f,col="blue")
plot(x,y,main="High order polynomial (y=bx^9)",pch=19,cex=1.25)
fit <- lm(y~poly(x,9))
fv <- predict(fit,data.frame(x=fr))
x2f <- predict(fit,data.frame(x=x2))
sqrt(mean((x2f-y2)^2))
lines(fr,fv, col='red', type='l',lwd=1.5) 
points(x2,y2,col="blue",pch=19,cex=1.25)
segments(x2,y2,x2,x2f,col="blue")
```

The new data points here are illustrated in blue, and the errors made by the model are illustrated by the blue lines between the red model fit and the new data points. Both models make errors, but the simpler quadratic model now makes smaller errors (RMSE = .064) than the 9th-degree polynomial (RMSE = .082). This reversal illustrates the unrealistic high performance promised by the more complex model when fitted without out-of-sample validation.

So how can we defend against cross-validation? We cannot simply always choose the simplest model, because - as have seen in the case of the linear fit above - it may not help us explain our data, or predict new data, at all. One way to reduce overfitting is to increase the size of your sample - you can see this illustrated below.

```{r fig.height=8, fig.width=8}
set.seed(1)
layout(matrix(1:4,2,2,byrow = T))
fr <- seq(-.5,.5,by = .001)


x <- runif(20)-.5
y <- x^2 + rnorm(20,0,.05)
plot(x,y,main="N = 20",pch=19,cex=1)
fit <- lm(y~x+I(x^2))
fv <- predict(fit,data.frame(x=fr))
lines(fr,fv, col='blue', type='l',lwd=3)
fit <- lm(y~poly(x,9))
fv <- predict(fit,data.frame(x=fr))
lines(fr,fv, col='red', type='l',lwd=3)

x <- runif(100)-.5
y <- x^2 + rnorm(100,0,.05)
plot(x,y,main="N = 100",pch=19,cex=1)
fit <- lm(y~x+I(x^2))
fv <- predict(fit,data.frame(x=fr))
lines(fr,fv, col='blue', type='l',lwd=3)
fit <- lm(y~poly(x,9))
fv <- predict(fit,data.frame(x=fr))
lines(fr,fv, col='red', type='l',lwd=3)

x <- runif(500)-.5
y <- x^2 + rnorm(500,0,.05)
plot(x,y,main="N = 500",pch=19,cex=1)
fit <- lm(y~x+I(x^2))
fv <- predict(fit,data.frame(x=fr))
lines(fr,fv, col='blue', type='l',lwd=3)
fit <- lm(y~poly(x,9))
fv <- predict(fit,data.frame(x=fr))
lines(fr,fv, col='red', type='l',lwd=3)

x <- runif(2500)-.5
y <- x^2 + rnorm(2500,0,.05)
plot(x,y,main="N = 2500",pch=19,cex=1)
fit <- lm(y~x+I(x^2))
fv <- predict(fit,data.frame(x=fr))
lines(fr,fv, col='blue', type='l',lwd=3)
fit <- lm(y~poly(x,9))
fv <- predict(fit,data.frame(x=fr))
lines(fr,fv, col='red', type='l',lwd=3)

```

As you can see, as the sample size increases, the overly-complex 9th-degree polynomial gradually converges on the simpler quadratic fit which better explains the data-generating process. However, while increasing one's sample size is generally desireable (all-else-equal) it is often impractical to do so beyond a certain point due to practical constraints such as time and money. How can we know whether our model is disasterously overfitting our data with the largest sample we can afford to collect? This is where cross-validation comes in.


## Basic forms of cross-validation

Out-of-sample model validation is a way to estimate the generalization performance of a statistical model. The simplest form of validation is to train a model on one data set, and then test it on another, independent, data set. So where does the "cross" in cross-validation come from? In cross-validation, this validation procedure is repeated, treating each independent data set (or sample from within a single data set) as both the training and the testing sequence in turn.

The simplest form of cross-validation is two divide the data set into two even halves. The model is first trained on half one, and then tested on half two. Alternatively, 

![Illustration of 4-fold cross-validation by Fabian Flöck (CC-BY-SA)](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)




